{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f6beae",
   "metadata": {},
   "source": [
    "# Análisis Exploratorio de Datos (EDA) - Dataset de Compras Black Sales\n",
    "\n",
    "## 2a) EDA y preparación del dataset\n",
    "\n",
    "En este análisis vamos a:\n",
    "1. Explorar el dataset de compras realizadas durante un black sales\n",
    "2. Realizar el pre-procesamiento necesario\n",
    "3. Crear segmentos de clientes basados en el gasto\n",
    "4. Preparar los datos para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import time\n",
    "import torch\n",
    "import copy\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bd16d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89dbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('data/dataset_compras.csv')\n",
    "\n",
    "# Información básica del dataset\n",
    "print(\"Información general del dataset:\")\n",
    "print(f\"Dimensiones: {df.shape}\")\n",
    "print(f\"Filas: {df.shape[0]:,}\")\n",
    "print(f\"Columnas: {df.shape[1]}\")\n",
    "print(f\"\\nTipos de datos:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nPrimeras 5 filas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea6c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estadísticas descriptivas:\")\n",
    "print(\"Variables numéricas:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nInformación de columnas categóricas:\")\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    print(f\"{col}: {df[col].nunique()} valores únicos\")\n",
    "    print(f\"  Valores: {df[col].unique()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones de las distribuciones\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Distribución de Purchase (variable objetivo)\n",
    "axes[0, 0].hist(df['Purchase'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Distribución de Purchase')\n",
    "axes[0, 0].set_xlabel('Purchase Amount')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Distribución por Género\n",
    "gender_counts = df['Gender'].value_counts()\n",
    "axes[0, 1].bar(gender_counts.index, gender_counts.values)\n",
    "axes[0, 1].set_title('Distribución por Género')\n",
    "axes[0, 1].set_xlabel('Género')\n",
    "axes[0, 1].set_ylabel('Cantidad')\n",
    "\n",
    "# Distribución por Edad\n",
    "age_counts = df['Age'].value_counts()\n",
    "axes[0, 2].bar(age_counts.index, age_counts.values, color='orange')\n",
    "axes[0, 2].set_title('Distribución por Edad')\n",
    "axes[0, 2].set_xlabel('Grupo de Edad')\n",
    "axes[0, 2].set_ylabel('Cantidad')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Distribución por Estado Civil\n",
    "marital_counts = df['Marital_Status'].value_counts()\n",
    "axes[1, 0].bar(['Soltero', 'Casado'], marital_counts.values, color='green')\n",
    "axes[1, 0].set_title('Distribución por Estado Civil')\n",
    "axes[1, 0].set_xlabel('Estado Civil')\n",
    "axes[1, 0].set_ylabel('Cantidad')\n",
    "\n",
    "# Distribución por Categoría de Ciudad\n",
    "city_counts = df['City_Category'].value_counts()\n",
    "axes[1, 1].bar(city_counts.index, city_counts.values, color='red')\n",
    "axes[1, 1].set_title('Distribución por Categoría de Ciudad')\n",
    "axes[1, 1].set_xlabel('Categoría de Ciudad')\n",
    "axes[1, 1].set_ylabel('Cantidad')\n",
    "\n",
    "# Años en la ciudad actual\n",
    "years_counts = df['Stay_In_Current_City_Years'].value_counts()\n",
    "axes[1, 2].bar(years_counts.index, years_counts.values, color='purple')\n",
    "axes[1, 2].set_title('Años en Ciudad Actual')\n",
    "axes[1, 2].set_xlabel('Años')\n",
    "axes[1, 2].set_ylabel('Cantidad')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b55850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear segmentos de clientes basados en Purchase\n",
    "def crear_segmento(purchase):\n",
    "    if purchase < 3000:\n",
    "        return 'Casual'\n",
    "    elif 3000 <= purchase < 8000:\n",
    "        return 'Gama_media'\n",
    "    elif 8000 <= purchase < 18000:\n",
    "        return 'Gama_alta'\n",
    "    else:\n",
    "        return 'Premium'\n",
    "\n",
    "# Aplicar la función para crear la nueva columna de segmentos\n",
    "df['Segmento'] = df['Purchase'].apply(crear_segmento)\n",
    "\n",
    "# Analizar la distribución de segmentos\n",
    "segmento_counts = df['Segmento'].value_counts()\n",
    "segmento_percentage = (segmento_counts / len(df)) * 100\n",
    "\n",
    "print(\"Distribución de segmentos:\")\n",
    "for segmento, count in segmento_counts.items():\n",
    "    pct = segmento_percentage[segmento]\n",
    "    print(f\"{segmento}: {count:,} clientes ({pct:.1f}%)\")\n",
    "\n",
    "# Visualización de la distribución de segmentos\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Gráfico de barras\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(segmento_counts.index, segmento_counts.values, \n",
    "        color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "plt.title('Distribución de Segmentos de Clientes')\n",
    "plt.xlabel('Segmento')\n",
    "plt.ylabel('Cantidad de Clientes')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Gráfico circular\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "plt.pie(segmento_counts.values, labels=segmento_counts.index, autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90)\n",
    "plt.title('Proporción de Segmentos de Clientes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de correlación\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numeric_columns].corr()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Heatmap de correlación completo\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "            fmt='.3f', square=True, ax=axes[0,0], cbar_kws={'shrink': .8})\n",
    "axes[0,0].set_title('Matriz de Correlación Completa', fontsize=16, fontweight='bold', pad=20)\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Correlaciones con Purchase\n",
    "purchase_corr = correlation_matrix['Purchase'].abs().sort_values(ascending=False)\n",
    "purchase_corr_display = purchase_corr[purchase_corr.index != 'Purchase'].head(10)\n",
    "\n",
    "colors = ['red' if correlation_matrix['Purchase'][var] < 0 else 'blue' for var in purchase_corr_display.index]\n",
    "bars = axes[0,1].barh(range(len(purchase_corr_display)), purchase_corr_display.values, color=colors, alpha=0.7)\n",
    "axes[0,1].set_yticks(range(len(purchase_corr_display)))\n",
    "axes[0,1].set_yticklabels(purchase_corr_display.index)\n",
    "axes[0,1].set_xlabel('Correlación Absoluta con Purchase', fontweight='bold')\n",
    "axes[0,1].set_title('Top 10 Variables Más Correlacionadas con Purchase', fontsize=16, fontweight='bold', pad=20)\n",
    "axes[0,1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, purchase_corr_display.values)):\n",
    "    actual_corr = correlation_matrix['Purchase'][purchase_corr_display.index[i]]\n",
    "    axes[0,1].text(val + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{actual_corr:.3f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Distribución de valores de correlación\n",
    "corr_values = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)]\n",
    "axes[1,0].hist(corr_values, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1,0].axvline(np.mean(corr_values), color='red', linestyle='--', \n",
    "                  label=f'Media: {np.mean(corr_values):.3f}')\n",
    "axes[1,0].axvline(np.median(corr_values), color='orange', linestyle='--', \n",
    "                  label=f'Mediana: {np.median(corr_values):.3f}')\n",
    "axes[1,0].set_xlabel('Valor de Correlación')\n",
    "axes[1,0].set_ylabel('Frecuencia')\n",
    "axes[1,0].set_title('Distribución de Coeficientes de Correlación', fontsize=16, fontweight='bold', pad=20)\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Correlación con Segmento\n",
    "if 'Segmento' in df.columns:\n",
    "    segmento_encoder = LabelEncoder()\n",
    "    df_temp = df.copy()\n",
    "    df_temp['Segmento_encoded'] = segmento_encoder.fit_transform(df['Segmento'])\n",
    "    \n",
    "    numeric_cols_with_segmento = df_temp.select_dtypes(include=[np.number]).columns\n",
    "    corr_with_segmento = df_temp[numeric_cols_with_segmento].corr()['Segmento_encoded'].abs().sort_values(ascending=False)\n",
    "    corr_with_segmento = corr_with_segmento[corr_with_segmento.index != 'Segmento_encoded'].head(8)\n",
    "    \n",
    "    colors_seg = ['green' if df_temp[numeric_cols_with_segmento].corr()['Segmento_encoded'][var] > 0 else 'orange' \n",
    "                  for var in corr_with_segmento.index]\n",
    "    \n",
    "    bars_seg = axes[1,1].barh(range(len(corr_with_segmento)), corr_with_segmento.values, \n",
    "                              color=colors_seg, alpha=0.7)\n",
    "    axes[1,1].set_yticks(range(len(corr_with_segmento)))\n",
    "    axes[1,1].set_yticklabels(corr_with_segmento.index)\n",
    "    axes[1,1].set_xlabel('Correlación Absoluta con Segmento', fontweight='bold')\n",
    "    axes[1,1].set_title('Variables Más Correlacionadas con Segmento de Cliente', fontsize=16, fontweight='bold', pad=20)\n",
    "    axes[1,1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    for i, (bar, val) in enumerate(zip(bars_seg, corr_with_segmento.values)):\n",
    "        actual_corr_seg = df_temp[numeric_cols_with_segmento].corr()['Segmento_encoded'][corr_with_segmento.index[i]]\n",
    "        axes[1,1].text(val + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "                       f'{actual_corr_seg:.3f}', ha='left', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Análisis de valores nulos:\")\n",
    "\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Variable': df.columns,\n",
    "    'Total_Nulos': df.isnull().sum(),\n",
    "    'Porcentaje_Nulos': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "missing_analysis = missing_analysis.sort_values('Porcentaje_Nulos', ascending=False)\n",
    "\n",
    "print(\"Distribución de valores nulos:\")\n",
    "print(missing_analysis[missing_analysis['Total_Nulos'] > 0])\n",
    "\n",
    "vars_con_nulos = missing_analysis[missing_analysis['Total_Nulos'] > 0]['Variable'].tolist()\n",
    "\n",
    "if len(vars_con_nulos) >= 2:\n",
    "    df_nulos = df[vars_con_nulos].isnull()\n",
    "    patrones_nulos = df_nulos.value_counts().head(4)\n",
    "    print(\"\\nPatrones de valores nulos:\")\n",
    "    for patron, count in patrones_nulos.items():\n",
    "        print(f\"  {patron}: {count:,} casos ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "for var_nulo in vars_con_nulos:\n",
    "    if var_nulo not in ['User_ID', 'Product_ID']:\n",
    "        total_productos = df['Product_ID'].nunique()\n",
    "        productos_con_nulos = df[df[var_nulo].isnull()]['Product_ID'].nunique()\n",
    "        pct_productos_afectados = (productos_con_nulos / total_productos) * 100\n",
    "        \n",
    "        if pct_productos_afectados > 30:\n",
    "            tipo_nulo = \"MNAR - Sistematico\"\n",
    "        elif pct_productos_afectados > 80:\n",
    "            tipo_nulo = \"MAR - Dependiente\"\n",
    "        else:\n",
    "            tipo_nulo = \"MCAR - Aleatorio\"\n",
    "        \n",
    "        print(f\"{var_nulo}: {pct_productos_afectados:.1f}% productos afectados - {tipo_nulo}\")\n",
    "\n",
    "print(\"Análisis completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7159f",
   "metadata": {},
   "source": [
    "### 2b) Modelo de deep leaning sin embeddings (1.5 puntos)\n",
    "Entrenar un modelo de deep learning usando Pytorch que no utilice embeddings, **descartando el `product_id` y `user_id`**. Graficar las evoluciones por época de la función de costo y de métricas como el accuracy y el f1 score para train y validation. Mostrar una matriz de confusión absoluta y otra normalizada por fila. Explicar el proceso de iteracion utilizado para conseguir los resultados y justificar los resultados obtenidos.\n",
    "\n",
    "Pueden usarse herramientas de regularización y prueba de hiperpametros para conseguir mejores resultados.\n",
    "\n",
    "Un resultado aceptable sería al menos un 78% de accuracy y f1 score para el set de validation.\n",
    "\n",
    "## Modelo DeepCustomerSegmentationNet\n",
    "\n",
    "Vamos a aplicar técnicas avanzadas de regularización:\n",
    "1. **Regularización L1 + L2 (Elastic Net)**\n",
    "2. **Dropout mejorado con tasas progresivas**\n",
    "3. **Learning Rate Scheduler avanzado (OneCycleLR)**\n",
    "4. **Early Stopping más preciso**\n",
    "5. **Gradient Clipping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc952198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar características y variable objetivo\n",
    "X = df.drop(['Purchase', 'Segmento', 'User_ID', 'Product_ID'], axis=1)\n",
    "y = df['Segmento']\n",
    "\n",
    "# Dividir dataset: 70% train, 15% val, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Obtener índices correspondientes para cada conjunto\n",
    "train_indices = X_train.index\n",
    "val_indices = X_val.index\n",
    "test_indices = X_test.index\n",
    "\n",
    "# Calcular estadísticas del conjunto de entrenamiento\n",
    "train_product_frequency = df.loc[train_indices, 'Product_ID'].value_counts().to_dict()\n",
    "\n",
    "# Aplicar las estadísticas a todos los conjuntos\n",
    "default_frequency = 1 \n",
    "\n",
    "# Agregar Product_Frequency a cada conjunto\n",
    "X_train = X_train.copy()\n",
    "X_train['Product_Frequency'] = df.loc[train_indices, 'Product_ID'].map(train_product_frequency).fillna(default_frequency)\n",
    "\n",
    "X_val = X_val.copy()\n",
    "X_val['Product_Frequency'] = df.loc[val_indices, 'Product_ID'].map(train_product_frequency).fillna(default_frequency)\n",
    "\n",
    "X_test = X_test.copy()\n",
    "X_test['Product_Frequency'] = df.loc[test_indices, 'Product_ID'].map(train_product_frequency).fillna(default_frequency)\n",
    "\n",
    "# Verificar productos no vistos\n",
    "unseen_val = sum(df.loc[val_indices, 'Product_ID'].map(train_product_frequency).isna())\n",
    "unseen_test = sum(df.loc[test_indices, 'Product_ID'].map(train_product_frequency).isna())\n",
    "print(f\"Productos no vistos en entrenamiento - Val: {unseen_val}, Test: {unseen_test}\")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} | Validation: {X_val.shape[0]} | Test: {X_test.shape[0]}\")\n",
    "\n",
    "# Definir columnas por tipo\n",
    "numeric_cols = ['Marital_Status', 'Product_Category', 'Product_Subcategory_1', 'Product_Subcategory_2', \n",
    "                'Product_Frequency']\n",
    "categorical_nominal = ['Gender', 'City_Category']\n",
    "categorical_ordinal = ['Age', 'Stay_In_Current_City_Years']\n",
    "\n",
    "# Configurar transformadores\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_cols),\n",
    "        ('cat_nom', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_nominal),\n",
    "        ('cat_ord', OrdinalEncoder(\n",
    "            categories=[\n",
    "                ['0-17', '18-25', '26-35', '36-45', '46-50', '51-55', '55+'],\n",
    "                ['1', '2', '3', '4', '5+']\n",
    "            ],\n",
    "            handle_unknown='use_encoded_value', \n",
    "            unknown_value=-1\n",
    "        ), categorical_ordinal)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transformar datos\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_val_encoded = preprocessor.transform(X_val)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "# Imputación\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=-999)\n",
    "X_train_imputed = imputer.fit_transform(X_train_encoded)\n",
    "X_val_imputed = imputer.transform(X_val_encoded)\n",
    "X_test_imputed = imputer.transform(X_test_encoded)\n",
    "\n",
    "# Escalar datos\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_val_scaled = scaler.transform(X_val_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Codificar target\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "print(f\"Shape final - Features: {X_train_scaled.shape[1]} | Classes: {len(le.classes_)}\")\n",
    "print(f\"Clases: {le.classes_}\")\n",
    "print(f\"Distribución train: {np.bincount(y_train_encoded)}\")\n",
    "print(f\"Distribución val: {np.bincount(y_val_encoded)}\")\n",
    "print(f\"Distribución test: {np.bincount(y_test_encoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff20aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de configuración\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LR = 0.01\n",
    "WEIGHT_DECAY = 1e-4\n",
    "L1_LAMBDA = 1e-5\n",
    "PATIENCE = 10\n",
    "MAX_GRAD_NORM = 1.0\n",
    "LABEL_SMOOTHING = 0.1\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Arquitectura del modelo\n",
    "INPUT_SIZE = X_train_scaled.shape[1]\n",
    "HIDDEN_LAYERS = [512, 256, 128, 64, 32]\n",
    "NUM_CLASSES = 4\n",
    "DROPOUT_RATES = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "class CustomerDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.FloatTensor(x)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "class DeepCustomerSegmentationNet(nn.Module):\n",
    "    def __init__(self, input_size=INPUT_SIZE, hidden_layers=HIDDEN_LAYERS, \n",
    "                 num_classes=NUM_CLASSES, dropout_rates=DROPOUT_RATES):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        \n",
    "        prev_size = input_size\n",
    "        for i, (hidden_size, dropout_rate) in enumerate(zip(hidden_layers, dropout_rates)):\n",
    "            self.layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_size))\n",
    "            self.dropouts.append(nn.Dropout(dropout_rate))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        self.output_layer = nn.Linear(prev_size, num_classes)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer, bn, dropout in zip(self.layers, self.batch_norms, self.dropouts):\n",
    "            x = F.relu(bn(layer(x)))\n",
    "            x = dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositivo utilizado: {device}\")\n",
    "\n",
    "train_dataset = CustomerDataset(X_train_scaled, y_train_encoded)\n",
    "val_dataset = CustomerDataset(X_val_scaled, y_val_encoded)\n",
    "test_dataset = CustomerDataset(X_test_scaled, y_test_encoded)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "model = DeepCustomerSegmentationNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=PATIENCE, min_lr=1e-6)\n",
    "\n",
    "print(f\"Modelo creado con {sum(p.numel() for p in model.parameters())} parámetros\")\n",
    "print(f\"Arquitectura: {INPUT_SIZE} -> {' -> '.join(map(str, HIDDEN_LAYERS))} -> {NUM_CLASSES}\")\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    return acc, f1, precision, recall\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, l1_lambda=L1_LAMBDA):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss += l1_lambda * l1_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc, f1, precision, recall = compute_metrics(all_targets, all_preds)\n",
    "    return avg_loss, acc, f1, precision, recall\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(batch_y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc, f1, precision, recall = compute_metrics(all_targets, all_preds)\n",
    "    return avg_loss, acc, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = EPOCHS\n",
    "best_f1 = 0\n",
    "patience = PATIENCE\n",
    "patience_counter = 0\n",
    "best_model = None\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "\n",
    "print(f\"Iniciando entrenamiento por {epochs} épocas\")\n",
    "print(\"Epoch | Train Loss | Train Acc | Train F1 | Val Loss | Val Acc | Val F1 | LR\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, train_f1, train_prec, train_rec = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    val_loss, val_acc, val_f1, val_prec, val_rec = validate_epoch(\n",
    "        model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step(val_f1)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    metrics['train_loss'].append(train_loss)\n",
    "    metrics['train_acc'].append(train_acc)\n",
    "    metrics['train_f1'].append(train_f1)\n",
    "    metrics['train_precision'].append(train_prec)\n",
    "    metrics['train_recall'].append(train_rec)\n",
    "    metrics['val_loss'].append(val_loss)\n",
    "    metrics['val_acc'].append(val_acc)\n",
    "    metrics['val_f1'].append(val_f1)\n",
    "    metrics['val_precision'].append(val_prec)\n",
    "    metrics['val_recall'].append(val_rec)\n",
    "    metrics['learning_rate'].append(current_lr)\n",
    "    \n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f\"{epoch+1:5d} | {train_loss:10.4f} | {train_acc:9.4f} | \"\n",
    "              f\"{train_f1:8.4f} | {val_loss:8.4f} | {val_acc:7.4f} | {val_f1:6.4f} | {current_lr:.2e}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "test_loss, test_acc, test_f1, test_prec, test_rec = validate_epoch(\n",
    "    model, test_loader, criterion, device)\n",
    "\n",
    "torch.save(best_model, 'best_deep_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa4d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].plot(metrics['train_loss'], label='Train Loss', color='blue', alpha=0.7)\n",
    "axes[0, 0].plot(metrics['val_loss'], label='Validation Loss', color='red', alpha=0.7)\n",
    "axes[0, 0].set_title('Loss durante el entrenamiento')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(metrics['train_acc'], label='Train Accuracy', color='blue', alpha=0.7)\n",
    "axes[0, 1].plot(metrics['val_acc'], label='Validation Accuracy', color='red', alpha=0.7)\n",
    "axes[0, 1].axhline(y=0.78, color='green', linestyle='--', alpha=0.8, label='Objetivo 78%')\n",
    "axes[0, 1].set_title('Accuracy durante el entrenamiento')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(metrics['train_f1'], label='Train F1-Score', color='blue', alpha=0.7)\n",
    "axes[1, 0].plot(metrics['val_f1'], label='Validation F1-Score', color='red', alpha=0.7)\n",
    "axes[1, 0].axhline(y=0.78, color='green', linestyle='--', alpha=0.8, label='Objetivo 78%')\n",
    "axes[1, 0].set_title('F1-Score durante el entrenamiento')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('F1-Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "final_metrics = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
    "test_scores = [test_acc, test_f1, test_prec, test_rec]\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon']\n",
    "\n",
    "bars = axes[1, 1].bar(final_metrics, test_scores, color=colors, alpha=0.8)\n",
    "axes[1, 1].axhline(y=0.78, color='red', linestyle='--', alpha=0.8, label='Objetivo 78%')\n",
    "axes[1, 1].set_title('Métricas finales en conjunto de test')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, test_scores):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Entrenamiento del Modelo DeepCustomerSegmentationNet', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = []\n",
    "    for batch_x, _ in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        y_test_pred.extend(preds)\n",
    "\n",
    "print(\"Reporte de clasificación detallado:\")\n",
    "print(classification_report(y_test_encoded, y_test_pred, target_names=le.classes_, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54601940",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = []\n",
    "    for batch_x, _ in test_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        y_test_pred.extend(preds)\n",
    "\n",
    "y_test_pred = np.array(y_test_pred)\n",
    "y_test_labels = le.inverse_transform(y_test_pred)\n",
    "y_test_true_labels = le.inverse_transform(y_test_encoded)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Prediccion': y_test_labels,\n",
    "    'Real': y_test_true_labels,\n",
    "    'Correcto': y_test_labels == y_test_true_labels\n",
    "})\n",
    "\n",
    "test_accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
    "test_f1 = f1_score(y_test_encoded, y_test_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"F1-Score: {test_f1:.4f}\")\n",
    "print(f\"Distribución de predicciones:\")\n",
    "print(results_df['Prediccion'].value_counts().sort_index())\n",
    "print(f\"Primeras 20 predicciones:\")\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e285c1",
   "metadata": {},
   "source": [
    "### 2c) Modelo de deep leaning con embeddings (2 puntos)\n",
    "Entrenar un modelo de deep learning usando Pytorch que utilice **2 capas de embeddings**, una para los productos y otra para los usuarios. Graficar las evoluciones por época de la función de costo y de métricas como el accuracy y el f1 score para train y validation. Mostrar una matriz de confusión absoluta y otra normalizada por fila. Explicar el proceso de iteracion utilizado para conseguir los resultados y justificar los resultados obtenidos. Justificar la cantidad de dimensiones usada para los embeddings. **Comparar contra el modelo sin embeddings** y explicar el porqué de los resultados.\n",
    "\n",
    "Elegir la cantidad justa y necesaria de dimensiones para los embeddings y justificar el porqué.\n",
    "\n",
    "Pueden usarse herramientas de regularización y prueba de hiperpametros para conseguir mejores resultados.\n",
    "\n",
    "Un resultado aceptable sería al menos un 90% de accuracy y f1 score para el set de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a653eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparación datos con embeddings\n",
    "user_encoder = LabelEncoder()\n",
    "product_encoder = LabelEncoder()\n",
    "\n",
    "df_users = pd.Series(df['User_ID'].unique())\n",
    "df_products = pd.Series(df['Product_ID'].unique())\n",
    "\n",
    "user_encoder.fit(df_users)\n",
    "product_encoder.fit(df_products)\n",
    "\n",
    "X_embedding = df.drop(['Purchase', 'Segmento'], axis=1)\n",
    "y_embedding = df['Segmento']\n",
    "\n",
    "X_temp_emb, X_test_emb, y_temp_emb, y_test_emb = train_test_split(\n",
    "    X_embedding, y_embedding, test_size=0.15, random_state=42, stratify=y_embedding)\n",
    "X_train_emb, X_val_emb, y_train_emb, y_val_emb = train_test_split(\n",
    "    X_temp_emb, y_temp_emb, test_size=0.176, random_state=42, stratify=y_temp_emb)\n",
    "\n",
    "# Codificar usuarios y productos\n",
    "X_train_emb = X_train_emb.copy()\n",
    "X_train_emb['User_ID_encoded'] = user_encoder.transform(X_train_emb['User_ID'])\n",
    "X_train_emb['Product_ID_encoded'] = product_encoder.transform(X_train_emb['Product_ID'])\n",
    "\n",
    "X_val_emb = X_val_emb.copy()\n",
    "X_val_emb['User_ID_encoded'] = user_encoder.transform(X_val_emb['User_ID'])\n",
    "X_val_emb['Product_ID_encoded'] = product_encoder.transform(X_val_emb['Product_ID'])\n",
    "\n",
    "X_test_emb = X_test_emb.copy()\n",
    "X_test_emb['User_ID_encoded'] = user_encoder.transform(X_test_emb['User_ID'])\n",
    "X_test_emb['Product_ID_encoded'] = product_encoder.transform(X_test_emb['Product_ID'])\n",
    "\n",
    "# Agregar Product_Frequency\n",
    "train_indices_emb = X_train_emb.index\n",
    "val_indices_emb = X_val_emb.index\n",
    "test_indices_emb = X_test_emb.index\n",
    "\n",
    "train_product_frequency_emb = df.loc[train_indices_emb, 'Product_ID'].value_counts().to_dict()\n",
    "default_frequency = 1\n",
    "\n",
    "X_train_emb['Product_Frequency'] = df.loc[train_indices_emb, 'Product_ID'].map(train_product_frequency_emb).fillna(default_frequency)\n",
    "X_val_emb['Product_Frequency'] = df.loc[val_indices_emb, 'Product_ID'].map(train_product_frequency_emb).fillna(default_frequency)\n",
    "X_test_emb['Product_Frequency'] = df.loc[test_indices_emb, 'Product_ID'].map(train_product_frequency_emb).fillna(default_frequency)\n",
    "\n",
    "# Preparar características \n",
    "categorical_cols = ['Gender', 'Age', 'City_Category', 'Stay_In_Current_City_Years']\n",
    "numeric_embedding_cols = ['Marital_Status', 'Product_Category', 'Product_Subcategory_1', 'Product_Subcategory_2', 'Product_Frequency']\n",
    "\n",
    "preprocessor_emb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_embedding_cols),\n",
    "        ('cat_nom', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), ['Gender', 'City_Category']),\n",
    "        ('cat_ord', OrdinalEncoder(\n",
    "            categories=[\n",
    "                ['0-17', '18-25', '26-35', '36-45', '46-50', '51-55', '55+'],\n",
    "                ['1', '2', '3', '4', '5+']\n",
    "            ],\n",
    "            handle_unknown='use_encoded_value', \n",
    "            unknown_value=-1\n",
    "        ), ['Age', 'Stay_In_Current_City_Years'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_cat = preprocessor_emb.fit_transform(X_train_emb[categorical_cols + numeric_embedding_cols])\n",
    "X_val_cat = preprocessor_emb.transform(X_val_emb[categorical_cols + numeric_embedding_cols])\n",
    "X_test_cat = preprocessor_emb.transform(X_test_emb[categorical_cols + numeric_embedding_cols])\n",
    "\n",
    "imputer_emb = SimpleImputer(strategy='constant', fill_value=-999)\n",
    "X_train_cat = imputer_emb.fit_transform(X_train_cat)\n",
    "X_val_cat = imputer_emb.transform(X_val_cat)\n",
    "X_test_cat = imputer_emb.transform(X_test_cat)\n",
    "\n",
    "scaler_emb = StandardScaler()\n",
    "X_train_cat_scaled = scaler_emb.fit_transform(X_train_cat)\n",
    "X_val_cat_scaled = scaler_emb.transform(X_val_cat)\n",
    "X_test_cat_scaled = scaler_emb.transform(X_test_cat)\n",
    "\n",
    "y_train_emb_encoded = le.transform(y_train_emb)\n",
    "y_val_emb_encoded = le.transform(y_val_emb)\n",
    "y_test_emb_encoded = le.transform(y_test_emb)\n",
    "\n",
    "NUM_USERS = len(user_encoder.classes_)\n",
    "NUM_PRODUCTS = len(product_encoder.classes_)\n",
    "USER_EMB_DIM = min(50, int(NUM_USERS**0.18))\n",
    "PRODUCT_EMB_DIM = min(50, int(NUM_PRODUCTS**0.15))\n",
    "CATEGORICAL_FEATURES = X_train_cat_scaled.shape[1]\n",
    "\n",
    "print(f\"Usuarios: {NUM_USERS} | Productos: {NUM_PRODUCTS}\")\n",
    "print(f\"Embedding dims - User: {USER_EMB_DIM} | Product: {PRODUCT_EMB_DIM}\")\n",
    "print(f\"Features categoricas: {CATEGORICAL_FEATURES}\")\n",
    "print(f\"Shapes finales - Train: {X_train_cat_scaled.shape}, Val: {X_val_cat_scaled.shape}, Test: {X_test_cat_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0cbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de configuración\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LR = 0.01\n",
    "WEIGHT_DECAY = 1e-4\n",
    "L1_LAMBDA = 1e-3\n",
    "PATIENCE = 3\n",
    "MAX_GRAD_NORM = 1.0\n",
    "LABEL_SMOOTHING = 0.1\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositivo utilizado: {device}\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, user_ids, product_ids, categorical_features, targets):\n",
    "        self.user_ids = torch.LongTensor(user_ids)\n",
    "        self.product_ids = torch.LongTensor(product_ids)\n",
    "        self.categorical_features = torch.FloatTensor(categorical_features)\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.user_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.user_ids[idx], self.product_ids[idx], \n",
    "                self.categorical_features[idx], self.targets[idx])\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, num_users, num_products, user_emb_dim, product_emb_dim, \n",
    "                 num_categorical_features, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, user_emb_dim)\n",
    "        self.product_embedding = nn.Embedding(num_products, product_emb_dim)\n",
    "        \n",
    "        total_input_size = user_emb_dim + product_emb_dim + num_categorical_features\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(total_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(), \n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.6),\n",
    "            \n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Embedding):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, user_ids, product_ids, categorical_features):\n",
    "        user_emb = self.user_embedding(user_ids)\n",
    "        product_emb = self.product_embedding(product_ids)\n",
    "        \n",
    "        x = torch.cat([user_emb, product_emb, categorical_features], dim=1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_emb = EmbeddingDataset(\n",
    "    X_train_emb['User_ID_encoded'].values,\n",
    "    X_train_emb['Product_ID_encoded'].values,\n",
    "    X_train_cat_scaled,\n",
    "    y_train_emb_encoded\n",
    ")\n",
    "\n",
    "val_dataset_emb = EmbeddingDataset(\n",
    "    X_val_emb['User_ID_encoded'].values,\n",
    "    X_val_emb['Product_ID_encoded'].values,\n",
    "    X_val_cat_scaled,\n",
    "    y_val_emb_encoded\n",
    ")\n",
    "\n",
    "test_dataset_emb = EmbeddingDataset(\n",
    "    X_test_emb['User_ID_encoded'].values,\n",
    "    X_test_emb['Product_ID_encoded'].values,\n",
    "    X_test_cat_scaled,\n",
    "    y_test_emb_encoded\n",
    ")\n",
    "\n",
    "train_loader_emb = DataLoader(train_dataset_emb, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader_emb = DataLoader(val_dataset_emb, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader_emb = DataLoader(test_dataset_emb, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "model_emb = EmbeddingNet(NUM_USERS, NUM_PRODUCTS, USER_EMB_DIM, PRODUCT_EMB_DIM, \n",
    "                        CATEGORICAL_FEATURES).to(device)\n",
    "\n",
    "criterion_emb = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer_emb = torch.optim.AdamW(model_emb.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler_emb = ReduceLROnPlateau(optimizer_emb, mode='max', factor=0.5, patience=PATIENCE, min_lr=1e-6)\n",
    "\n",
    "print(\"Dataset con embeddings configurado:\")\n",
    "print(f\"Train: {len(train_dataset_emb):,} muestras\")\n",
    "print(f\"Validation: {len(val_dataset_emb):,} muestras\") \n",
    "print(f\"Test: {len(test_dataset_emb):,} muestras\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Total parámetros modelo: {sum(p.numel() for p in model_emb.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a070a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_emb(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    for user_ids, product_ids, categorical_features, targets in loader:\n",
    "        user_ids = user_ids.to(device)\n",
    "        product_ids = product_ids.to(device) \n",
    "        categorical_features = categorical_features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_ids, product_ids, categorical_features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss += 1e-5 * l1_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc, f1, precision, recall = compute_metrics(all_targets, all_preds)\n",
    "    return avg_loss, acc, f1, precision, recall\n",
    "\n",
    "def validate_epoch_emb(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_ids, product_ids, categorical_features, targets in loader:\n",
    "            user_ids = user_ids.to(device)\n",
    "            product_ids = product_ids.to(device)\n",
    "            categorical_features = categorical_features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(user_ids, product_ids, categorical_features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc, f1, precision, recall = compute_metrics(all_targets, all_preds)\n",
    "    return avg_loss, acc, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaea51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_emb = EPOCHS\n",
    "best_f1_emb = 0\n",
    "patience_emb = PATIENCE\n",
    "patience_counter_emb = 0\n",
    "best_model_emb = None\n",
    "\n",
    "metrics_emb = defaultdict(list)\n",
    "\n",
    "print(f\"Entrenamiento modelo con embeddings - {epochs_emb} épocas\")\n",
    "print(\"Epoch | Train Loss | Train Acc | Train F1 | Val Loss | Val Acc | Val F1\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for epoch in range(epochs_emb):\n",
    "    train_loss_emb, train_acc_emb, train_f1_emb, train_prec_emb, train_rec_emb = train_epoch_emb(\n",
    "        model_emb, train_loader_emb, criterion_emb, optimizer_emb)\n",
    "    \n",
    "    val_loss_emb, val_acc_emb, val_f1_emb, val_prec_emb, val_rec_emb = validate_epoch_emb(\n",
    "        model_emb, val_loader_emb, criterion_emb)\n",
    "    \n",
    "    scheduler_emb.step(val_f1_emb)\n",
    "    \n",
    "    metrics_emb['train_loss'].append(train_loss_emb)\n",
    "    metrics_emb['train_acc'].append(train_acc_emb)\n",
    "    metrics_emb['train_f1'].append(train_f1_emb)\n",
    "    metrics_emb['val_loss'].append(val_loss_emb)\n",
    "    metrics_emb['val_acc'].append(val_acc_emb)\n",
    "    metrics_emb['val_f1'].append(val_f1_emb)\n",
    "    \n",
    "    if val_f1_emb > best_f1_emb:\n",
    "        best_f1_emb = val_f1_emb\n",
    "        best_model_emb = copy.deepcopy(model_emb.state_dict())\n",
    "        patience_counter_emb = 0\n",
    "    else:\n",
    "        patience_counter_emb += 1\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f\"{epoch+1:5d} | {train_loss_emb:10.4f} | {train_acc_emb:9.4f} | \"\n",
    "              f\"{train_f1_emb:8.4f} | {val_loss_emb:8.4f} | {val_acc_emb:7.4f} | {val_f1_emb:6.4f}\")\n",
    "    \n",
    "    if patience_counter_emb >= patience_emb:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "model_emb.load_state_dict(best_model_emb)\n",
    "test_loss_emb, test_acc_emb, test_f1_emb, test_prec_emb, test_rec_emb = validate_epoch_emb(\n",
    "    model_emb, test_loader_emb, criterion_emb)\n",
    "\n",
    "torch.save(best_model_emb, 'best_improved_model.pth')\n",
    "\n",
    "print(f\"Resultados finales:\")\n",
    "print(f\"Sin embeddings  - Test Acc: {test_acc:.4f} | Test F1: {test_f1:.4f}\")\n",
    "print(f\"Con embeddings  - Test Acc: {test_acc_emb:.4f} | Test F1: {test_f1_emb:.4f}\")\n",
    "print(f\"Mejora          - Acc: {test_acc_emb-test_acc:+.4f} | F1: {test_f1_emb-test_f1:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones y matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "axes[0, 0].plot(metrics_emb['train_loss'], label='Train Loss', color='blue', alpha=0.7)\n",
    "axes[0, 0].plot(metrics_emb['val_loss'], label='Validation Loss', color='red', alpha=0.7)\n",
    "axes[0, 0].set_title('Loss - Modelo con Embeddings')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(metrics_emb['train_acc'], label='Train Accuracy', color='blue', alpha=0.7)\n",
    "axes[0, 1].plot(metrics_emb['val_acc'], label='Validation Accuracy', color='red', alpha=0.7)\n",
    "axes[0, 1].axhline(y=0.90, color='green', linestyle='--', alpha=0.8, label='Objetivo 90%')\n",
    "axes[0, 1].set_title('Accuracy - Modelo con Embeddings')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 2].plot(metrics_emb['train_f1'], label='Train F1-Score', color='blue', alpha=0.7)\n",
    "axes[0, 2].plot(metrics_emb['val_f1'], label='Validation F1-Score', color='red', alpha=0.7)\n",
    "axes[0, 2].axhline(y=0.90, color='green', linestyle='--', alpha=0.8, label='Objetivo 90%')\n",
    "axes[0, 2].set_title('F1-Score - Modelo con Embeddings')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('F1-Score')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "model_emb.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred_emb = []\n",
    "    for user_ids, product_ids, categorical_features, _ in test_loader_emb:\n",
    "        user_ids = user_ids.to(device)\n",
    "        product_ids = product_ids.to(device)\n",
    "        categorical_features = categorical_features.to(device)\n",
    "        outputs = model_emb(user_ids, product_ids, categorical_features)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        y_test_pred_emb.extend(preds)\n",
    "\n",
    "cm_absolute = confusion_matrix(y_test_emb_encoded, y_test_pred_emb)\n",
    "cm_normalized = confusion_matrix(y_test_emb_encoded, y_test_pred_emb, normalize='true')\n",
    "\n",
    "im1 = axes[1, 0].imshow(cm_absolute, interpolation='nearest', cmap='Blues')\n",
    "axes[1, 0].set_title('Matriz de Confusión Absoluta')\n",
    "axes[1, 0].set_xlabel('Predicción')\n",
    "axes[1, 0].set_ylabel('Real')\n",
    "axes[1, 0].set_xticks(range(4))\n",
    "axes[1, 0].set_yticks(range(4))\n",
    "axes[1, 0].set_xticklabels(le.classes_)\n",
    "axes[1, 0].set_yticklabels(le.classes_)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[1, 0].text(j, i, str(cm_absolute[i, j]), ha='center', va='center', \n",
    "                       color='white' if cm_absolute[i, j] > cm_absolute.max()/2 else 'black')\n",
    "\n",
    "im2 = axes[1, 1].imshow(cm_normalized, interpolation='nearest', cmap='Blues')\n",
    "axes[1, 1].set_title('Matriz de Confusión Normalizada')\n",
    "axes[1, 1].set_xlabel('Predicción')\n",
    "axes[1, 1].set_ylabel('Real')\n",
    "axes[1, 1].set_xticks(range(4))\n",
    "axes[1, 1].set_yticks(range(4))\n",
    "axes[1, 1].set_xticklabels(le.classes_)\n",
    "axes[1, 1].set_yticklabels(le.classes_)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[1, 1].text(j, i, f'{cm_normalized[i, j]:.3f}', ha='center', va='center',\n",
    "                       color='white' if cm_normalized[i, j] > 0.5 else 'black')\n",
    "\n",
    "comparison_metrics = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
    "sin_emb_scores = [test_acc, test_f1, test_prec, test_rec]\n",
    "con_emb_scores = [test_acc_emb, test_f1_emb, test_prec_emb, test_rec_emb]\n",
    "\n",
    "x_pos = np.arange(len(comparison_metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 2].bar(x_pos - width/2, sin_emb_scores, width, \n",
    "                      label='Sin Embeddings', alpha=0.8, color='lightcoral')\n",
    "bars2 = axes[1, 2].bar(x_pos + width/2, con_emb_scores, width,\n",
    "                      label='Con Embeddings', alpha=0.8, color='lightblue')\n",
    "\n",
    "axes[1, 2].axhline(y=0.90, color='green', linestyle='--', alpha=0.8, label='Objetivo 90%')\n",
    "axes[1, 2].set_title('Comparación de Modelos')\n",
    "axes[1, 2].set_xlabel('Métricas')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_xticks(x_pos)\n",
    "axes[1, 2].set_xticklabels(comparison_metrics)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "for bars, scores in [(bars1, sin_emb_scores), (bars2, con_emb_scores)]:\n",
    "    for bar, score in zip(bars, scores):\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.suptitle('Modelo con Embeddings - Análisis Completo', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Reporte de clasificación - Modelo con Embeddings:\")\n",
    "print(classification_report(y_test_emb_encoded, y_test_pred_emb, target_names=le.classes_, digits=4))\n",
    "\n",
    "print(f\"Justificación de dimensiones de embeddings:\")\n",
    "print(f\"- User embedding: {USER_EMB_DIM} dimensiones para {NUM_USERS} usuarios\")\n",
    "print(f\"  Fórmula: min(50, usuarios^0.18) = min(50, {NUM_USERS}^0.18) ≈ {int(NUM_USERS**0.18)}\")\n",
    "print(f\"- Product embedding: {PRODUCT_EMB_DIM} dimensiones para {NUM_PRODUCTS} productos\") \n",
    "print(f\"  Fórmula: min(50, productos^0.15) = min(50, {NUM_PRODUCTS}^0.15) ≈ {int(NUM_PRODUCTS**0.15)}\")\n",
    "print(f\"Razón: Dimensiones suficientes para capturar patrones sin sobreajuste\")\n",
    "\n",
    "print(f\"Explicación de resultados:\")\n",
    "mejora_acc = test_acc_emb - test_acc\n",
    "mejora_f1 = test_f1_emb - test_f1\n",
    "if mejora_acc > 0 and mejora_f1 > 0:\n",
    "    print(f\"Los embeddings mejoran el rendimiento:\")\n",
    "    print(f\"  - Capturan patrones latentes usuario-producto\")\n",
    "    print(f\"  - Reducen la dimensionalidad categórica\")\n",
    "    print(f\"  - Mejor representación de entidades\")\n",
    "else:\n",
    "    print(f\"Embeddings no mejoran significativamente:\")\n",
    "    print(f\"  - Dataset puede ser demasiado pequeño\")\n",
    "    print(f\"  - Patrones ya capturados por features categóricas\")\n",
    "    print(f\"  - Necesita más regularización o ajuste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb390e28",
   "metadata": {},
   "source": [
    "## Justificación de Dimensiones de Embeddings\n",
    "\n",
    "### Dimensiones Elegidas:\n",
    "- **User Embedding**: 4 dimensiones para 7,432 usuarios\n",
    "- **Product Embedding**: 3 dimensiones para 2,455 productos\n",
    "\n",
    "### Fórmula utilizada:\n",
    "```\n",
    "USER_EMB_DIM = min(50, int(NUM_USERS**0.18))\n",
    "PRODUCT_EMB_DIM = min(50, int(NUM_PRODUCTS**0.15))\n",
    "```\n",
    "\n",
    "### Justificación:\n",
    "1. **Evitar overfitting**: Dimensiones pequeñas previenen memorización\n",
    "2. **Eficiencia computacional**: Menos parámetros = entrenamiento más rápido\n",
    "3. **Generalización**: Fuerza al modelo a aprender patrones generales\n",
    "4. **Regla empírica**: Raíz cuarta del número de entidades es un buen punto de partida\n",
    "\n",
    "## Herramientas de Regularización Utilizadas\n",
    "\n",
    "### ¿Por qué usar regularización?\n",
    "- **Prevenir overfitting**: Evita que el modelo memorice los datos de entrenamiento\n",
    "- **Mejorar generalización**: Permite mejor rendimiento en datos no vistos\n",
    "- **Estabilizar entrenamiento**: Reduce la varianza en el aprendizaje\n",
    "\n",
    "### Técnicas aplicadas:\n",
    "\n",
    "1. **Dropout (0.4-0.6)**: Desactiva neuronas aleatoriamente durante entrenamiento\n",
    "2. **L1 Regularization (1e-5)**: Penaliza pesos grandes, promueve sparsity\n",
    "3. **Weight Decay (1e-4)**: Regularización L2 en el optimizador\n",
    "4. **Label Smoothing (0.1)**: Suaviza las etiquetas objetivo\n",
    "5. **Gradient Clipping (1.0)**: Previene gradientes explosivos\n",
    "6. **Early Stopping (patience=3)**: Detiene entrenamiento cuando no hay mejora\n",
    "7. **Batch Normalization**: Normaliza activaciones entre capas\n",
    "\n",
    "### Resultados:\n",
    "- Sin regularización: Overfitting severo (100% accuracy)\n",
    "- Con regularización: Modelo más estable y generalizable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d779a",
   "metadata": {},
   "source": [
    "### 2d) Encontrar usuarios similares (1 punto)\n",
    "Para el modelo del punto 2c) implementar una función que reciba un ID de usuario y sugiera **n** cantidad de usuarios que tuvieron un comportamiento de compras similar. También se debe mostrar el grado o porcentaje de similitud de cada usuario que retorne la función."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_usuarios_similares(user_id, n=5, metrica='coseno'):\n",
    "    \"\"\"\n",
    "    Encuentra usuarios con comportamiento similar basado en embeddings.\n",
    "    \n",
    "    Args:\n",
    "        user_id: ID del usuario de referencia\n",
    "        n: Número de usuarios similares a retornar\n",
    "        metrica: 'coseno', 'producto_punto' o 'euclidiana'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con usuarios similares y porcentajes de similitud\n",
    "    \"\"\"\n",
    "    if user_id not in user_encoder.classes_:\n",
    "        return f\"Usuario {user_id} no encontrado en el dataset\"\n",
    "    \n",
    "    user_idx = user_encoder.transform([user_id])[0]\n",
    "    \n",
    "    model_emb.eval()\n",
    "    with torch.no_grad():\n",
    "        user_embeddings = model_emb.user_embedding.weight.cpu().numpy()\n",
    "        user_ref_embedding = user_embeddings[user_idx].reshape(1, -1)\n",
    "        \n",
    "        if metrica == 'coseno':\n",
    "            similarities = cosine_similarity(user_ref_embedding, user_embeddings)[0]\n",
    "        elif metrica == 'producto_punto':\n",
    "            similarities = np.dot(user_embeddings, user_ref_embedding.T).flatten()\n",
    "        elif metrica == 'euclidiana':\n",
    "            distances = euclidean_distances(user_ref_embedding, user_embeddings)[0]\n",
    "            similarities = 1 / (1 + distances)\n",
    "        else:\n",
    "            raise ValueError(\"Métrica no válida\")\n",
    "    \n",
    "    user_ids_decoded = user_encoder.classes_\n",
    "    results_df = pd.DataFrame({\n",
    "        'User_ID': user_ids_decoded,\n",
    "        'Similitud': similarities,\n",
    "        'Porcentaje': (similarities * 100).round(2) if metrica == 'coseno' else \n",
    "                     ((similarities - similarities.min()) / (similarities.max() - similarities.min()) * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    results_df = results_df[results_df['User_ID'] != user_id]\n",
    "    results_df = results_df.sort_values('Similitud', ascending=False).head(n)\n",
    "    \n",
    "    user_stats = []\n",
    "    for similar_user in results_df['User_ID'].values:\n",
    "        user_data = df[df['User_ID'] == similar_user]\n",
    "        stats = {\n",
    "            'Compras': len(user_data),\n",
    "            'Gasto_Medio': user_data['Purchase'].mean().round(2),\n",
    "            'Productos': user_data['Product_ID'].nunique(),\n",
    "            'Segmento': user_data['Segmento'].mode().iloc[0] if len(user_data) > 0 else 'N/A'\n",
    "        }\n",
    "        user_stats.append(stats)\n",
    "    \n",
    "    stats_df = pd.DataFrame(user_stats)\n",
    "    return pd.concat([results_df.reset_index(drop=True), stats_df], axis=1)\n",
    "\n",
    "def analizar_usuario(user_id):\n",
    "    \"\"\"Retorna estadísticas básicas del usuario.\"\"\"\n",
    "    user_data = df[df['User_ID'] == user_id]\n",
    "    return {\n",
    "        'compras': len(user_data),\n",
    "        'gasto_medio': user_data['Purchase'].mean().round(2),\n",
    "        'productos': user_data['Product_ID'].nunique(),\n",
    "        'segmento': user_data['Segmento'].mode().iloc[0]\n",
    "    }\n",
    "\n",
    "def comparar_usuarios(user_ref, usuarios_similares_df):\n",
    "    \"\"\"Compara usuario de referencia con usuarios similares.\"\"\"\n",
    "    ref_stats = analizar_usuario(user_ref)\n",
    "    \n",
    "    print(f\"Usuario de referencia: {user_ref}\")\n",
    "    print(f\"Compras: {ref_stats['compras']}, Gasto medio: {ref_stats['gasto_medio']}\")\n",
    "    print(f\"Productos únicos: {ref_stats['productos']}, Segmento: {ref_stats['segmento']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Usuarios similares:\")\n",
    "    for idx, row in usuarios_similares_df.iterrows():\n",
    "        print(f\"{row['User_ID']} - Similitud: {row['Porcentaje']:.1f}% - \"\n",
    "              f\"Compras: {row['Compras']} - Gasto: {row['Gasto_Medio']} - \"\n",
    "              f\"Segmento: {row['Segmento']}\")\n",
    "\n",
    "def evaluar_embeddings():\n",
    "    \"\"\"Evalúa la calidad de los embeddings generados.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        user_embeddings = model_emb.user_embedding.weight.cpu().numpy()\n",
    "        product_embeddings = model_emb.product_embedding.weight.cpu().numpy()\n",
    "    \n",
    "    similarities_matrix = cosine_similarity(user_embeddings)\n",
    "    np.fill_diagonal(similarities_matrix, np.nan)\n",
    "    \n",
    "    print(\"Estadísticas de embeddings:\")\n",
    "    print(f\"Usuarios: {user_embeddings.shape}, Productos: {product_embeddings.shape}\")\n",
    "    print(f\"Similitud promedio: {np.nanmean(similarities_matrix):.3f}\")\n",
    "    print(f\"Rango similitud: [{np.nanmin(similarities_matrix):.3f}, {np.nanmax(similarities_matrix):.3f}]\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "usuario_muestra = np.random.choice(df['User_ID'].unique())\n",
    "print(f\"Ejemplo con usuario: {usuario_muestra}\")\n",
    "\n",
    "usuarios_similares = encontrar_usuarios_similares(usuario_muestra, n=5)\n",
    "print(f\"Resultados similitud coseno:\")\n",
    "print(usuarios_similares[['User_ID', 'Porcentaje', 'Compras', 'Segmento']])\n",
    "\n",
    "print(f\"Comparación detallada:\")\n",
    "comparar_usuarios(usuario_muestra, usuarios_similares)\n",
    "\n",
    "print(f\"Evaluación de embeddings:\")\n",
    "evaluar_embeddings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
